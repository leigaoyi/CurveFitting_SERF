{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8baccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cases\n",
      "(64.023587929335, array([  3.56045757, -25.26654538]), 'Finished kmax iterations')\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Module: Levenberg-Marquardt Algorithm Implementations\n",
    "    Used for optimizing paramters to fit a distribtion.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy import inner, max, diag, eye, Inf, dot\n",
    "from numpy.linalg import norm, solve\n",
    "import time\n",
    "\n",
    "def line_with_noise(params, x, mu=0, sigma=5):\n",
    "    \"\"\" Calculate Line\n",
    "    :param params: parameters for line equation y = mx + b ([m, b])\n",
    "    :param x: input values\n",
    "    :return: a vector containing the output of the line equation with noise\n",
    "    \"\"\"\n",
    "\n",
    "    m, b = params[0:2]\n",
    "\n",
    "    noise = np.random.normal(mu, sigma, len(x))\n",
    "    y = m * x + b + noise\n",
    "    return y\n",
    "\n",
    "def line_differentiation(params, args):\n",
    "    \"\"\" Symbolic Differentiation for Line Equation\n",
    "    Note: we are passing in the effor function for the model we are using, but\n",
    "    we can substitute the error for the actual model function\n",
    "        error(x + delta) - error(x) <==> f(x + delta) - f(x)\n",
    "    :param params: values to be used in model\n",
    "    :param args: input (x) and observations (y)\n",
    "    :return: The jacobian for the error_function\n",
    "    \"\"\"\n",
    "\n",
    "    m, b = params\n",
    "    x, y = args\n",
    "\n",
    "    # Jacobian\n",
    "    J = np.empty(shape=(len(params),) + x.shape, dtype=np.float32)\n",
    "    J[0] = x  # d/dm = x\n",
    "    J[1] = 1  # d/db = 1\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def line_error(params, args):\n",
    "    \"\"\"\n",
    "    Line Error, calculates the error for the line equations y = mx + b\n",
    "    :param params: values to be used in model\n",
    "    :param x: inputs\n",
    "    :param y: observations\n",
    "    :return: difference between observations and estimates\n",
    "    \"\"\"\n",
    "    x, y = args\n",
    "    m, b = params[0:2]\n",
    "    y_star = m * x + b\n",
    "\n",
    "    return y - y_star\n",
    "\n",
    "def numerical_differentiation(params, args, error_function):\n",
    "    \"\"\" Numerical Differentiation\n",
    "    Note: we are passing in the effor function for the model we are using, but\n",
    "    we can substitute the error for the actual model function\n",
    "        error(x + delta) - error(x) <==> f(x + delta) - f(x)\n",
    "    :param params: values to be used in model\n",
    "    :param args: input (x) and observations (y)\n",
    "    :param error_function: function used to determine error based on params and observations\n",
    "    :return: The jacobian for the error_function\n",
    "    \"\"\"\n",
    "    delta_factor = 1e-4\n",
    "    min_delta = 1e-4\n",
    "\n",
    "    # Compute error\n",
    "    y_0 = error_function(params, args)\n",
    "\n",
    "    # Jacobian\n",
    "    J = np.empty(shape=(len(params),) + y_0.shape, dtype=np.float32)\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "        params_star = params[:]\n",
    "        delta = param * delta_factor\n",
    "\n",
    "        if abs(delta) < min_delta:\n",
    "            delta = min_delta\n",
    "\n",
    "        # Update single param and calculate error with updated value\n",
    "        params_star[i] += delta\n",
    "        y_1 = error_function(params_star, args)\n",
    "\n",
    "        # Update Jacobian with gradients\n",
    "        diff = y_0 - y_1\n",
    "        J[i] = diff / delta\n",
    "\n",
    "    return J\n",
    "\n",
    "def LM(seed_params, args,\n",
    "       error_function, jacobian_function=numerical_differentiation,\n",
    "       llambda=1e-3, lambda_multiplier=10, kmax=500, eps=1e-3, verbose=False):\n",
    "    \"\"\" Levenberg-Marquardt Implementaiton\n",
    "     See: (https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm)\n",
    "    :param  seed_params: initial starting guess for the params we are trying to find\n",
    "    :param  args: the inputs (x) and observations (y)\n",
    "    :param  error_function: describes how error is calculated for the model\n",
    "        function args (params, x, y)\n",
    "    :param  jacobian_function: produces and returns the jacobian for model\n",
    "        function args (params, args, error_function)\n",
    "    :param  llambda: initial dampening factor\n",
    "    :param  lambda_multiplier: scale used to increase/decrease lambda\n",
    "    :param  kmax: max number of iterations\n",
    "    :return:  rmserror, params\n",
    "    \"\"\"\n",
    "\n",
    "    # Equality : (JtJ + lambda * I * diag(JtJ)) * delta = Jt * error\n",
    "    # Solve for delta\n",
    "    params = seed_params\n",
    "\n",
    "    k = 0\n",
    "    while k < kmax:\n",
    "        k += 1\n",
    "\n",
    "        # Retrieve jacobian of function gradients with respect to the params\n",
    "        J = jacobian_function(params, args, error_function)\n",
    "        JtJ = inner(J, J)\n",
    "\n",
    "        # I * diag(JtJ)\n",
    "        A = eye(len(params)) * diag(JtJ)\n",
    "\n",
    "        # == Jt * error\n",
    "        error = error_function(params, args)\n",
    "        Jerror = inner(J, error)\n",
    "\n",
    "        rmserror = norm(error)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"{} RMS: {} Params: {}\".format(k, rmserror, params))\n",
    "\n",
    "        if rmserror < eps:\n",
    "            reason = \"Converged to min epsilon\"\n",
    "            return rmserror, params, reason\n",
    "\n",
    "        reason = \"\"\n",
    "        error_star = error[:]\n",
    "        rmserror_star = rmserror + 1\n",
    "        while rmserror_star >= rmserror:\n",
    "            try:\n",
    "                delta = solve(JtJ + llambda * A, Jerror)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Error: Singular Matrix\")\n",
    "                return -1\n",
    "\n",
    "            # Update params and calculate new error\n",
    "            params_star = params[:] + delta[:]\n",
    "            error_star = error_function(params_star, args)\n",
    "            rmserror_star = norm(error_star)\n",
    "\n",
    "            if rmserror_star < rmserror:\n",
    "                params = params_star\n",
    "                llambda /= lambda_multiplier\n",
    "                break\n",
    "\n",
    "            llambda *= lambda_multiplier\n",
    "\n",
    "            # Return if lambda explodes or if change is small\n",
    "            if llambda > 1e9:\n",
    "                reason = \"Lambda to large.\"\n",
    "                return rmserror, params, reason\n",
    "\n",
    "        reduction = abs(rmserror - rmserror_star)\n",
    "        if reduction < 1e-18:\n",
    "            reason = \"Change in error too small\"\n",
    "            return rmserror, params, reason\n",
    "\n",
    "    return rmserror, params, \"Finished kmax iterations\"\n",
    "\n",
    "def line_with_noise(params, x, mu=0, sigma=5):\n",
    "    \"\"\" Calculate Line\n",
    "    :param params: parameters for line equation y = mx + b ([m, b])\n",
    "    :param x: input values\n",
    "    :return: a vector containing the output of the line equation with noise\n",
    "    \"\"\"\n",
    "\n",
    "    m, b = params[0:2]\n",
    "\n",
    "    noise = np.random.normal(mu, sigma, len(x))\n",
    "    y = m * x + b + noise\n",
    "    return y\n",
    "\n",
    "\n",
    "def testLM():\n",
    "    #####################\n",
    "    # Test Line Fitting\n",
    "    #####################\n",
    "\n",
    "    # Input\n",
    "    x = np.linspace(-500, 500, 1001)\n",
    "\n",
    "    # Parameters:   m       b\n",
    "    line_params = [3.56, -25.36]\n",
    "\n",
    "    # Observations\n",
    "    y = line_with_noise(line_params, x, 0, 2)\n",
    "\n",
    "    # Seed\n",
    "    start_params = [0, 0]\n",
    "\n",
    "    return LM(start_params, (x, y), line_error, numerical_differentiation)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Test Cases\")\n",
    "    print(testLM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277c6061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a189107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func(x, a, b, omega, phi, c):\n",
    "    return a * np.exp(-b * x) * np.sin(omega * x + phi) + c\n",
    "\n",
    "def levenberg_marquardt_least_squares(X, y, f, jacobian, lam=0.01, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Implements the Levenberg-Marquardt algorithm for solving least squares problems.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): Independent variable data of shape (n_samples, n_features).\n",
    "    y (numpy array): Dependent variable data of shape (n_samples,).\n",
    "    f (function): Function that maps X to y.\n",
    "    jacobian (function): Function that returns the Jacobian matrix of f with respect to X.\n",
    "    lam (float): Initial damping parameter. Default is 0.01.\n",
    "    tol (float): Tolerance for stopping criterion. Default is 1e-6.\n",
    "    max_iter (int): Maximum number of iterations. Default is 100.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple (theta, J) containing the estimated parameters theta and the value of the cost function J.\n",
    "    \"\"\"\n",
    "    theta = np.zeros(X.shape[1]) # Initialize parameters to zeros\n",
    "    J = np.sum((y - f(X, *theta))**2) # Compute initial cost function value\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        J_prev = J\n",
    "        J = np.sum((y - f(X, *theta))**2) # Compute current cost function value\n",
    "        if abs(J - J_prev) < tol: # Stopping criterion\n",
    "            break\n",
    "        \n",
    "        # Compute Jacobian matrix\n",
    "        J_X = jacobian(X, *theta)\n",
    "        \n",
    "        # Compute update step\n",
    "        H = J_X.T @ J_X + lam * np.eye(X.shape[1]) # Compute Hessian matrix\n",
    "        g = J_X.T @ (y - f(X, *theta)) # Compute gradient vector\n",
    "        delta = np.linalg.solve(H, g) # Solve for delta\n",
    "        \n",
    "        # Update parameters and damping parameter\n",
    "        theta_new = theta + delta\n",
    "        J_new = np.sum((y - f(X, *theta_new))**2)\n",
    "        if J_new < J:\n",
    "            lam /= 10\n",
    "            theta = theta_new\n",
    "        else:\n",
    "            lam *= 10\n",
    "        \n",
    "    return theta, J\n",
    "\n",
    "# Generate synthetic data\n",
    "x = np.linspace(0, 10, 101)\n",
    "y = func(x, 1, 0.1, 1, 0, 0) + np.random.normal(0, 0.1, size=101)\n",
    "\n",
    "# Define functions\n",
    "def f(X, a, b, omega, phi, c):\n",
    "    return a * np.exp(-b * X[:,0]) * np.sin(omega * X[:,0] + phi) + c\n",
    "\n",
    "def jacobian(X, a, b, omega, phi, c):\n",
    "    J = np.zeros((X.shape[0], X.shape[1]))\n",
    "    J[:,0] = np.exp(-b * X[:,0]) * np.sin(omega * X[:,0] + phi)\n",
    "    J[:,1] = -a * X[:,0] * np.exp(-b * X[:,0]) * np.sin(omega * X[:,0] + phi)\n",
    "    J[:,2] = a * np.exp(-b * X[:,0]) * np.cos(omega * X[:,0] +\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
